{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_org= pd.read_csv(\"Twitter-Absolute-Sigma-500.data\",sep=',',header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the first 5 rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>889</td>\n",
       "      <td>939</td>\n",
       "      <td>960</td>\n",
       "      <td>805</td>\n",
       "      <td>805</td>\n",
       "      <td>1143</td>\n",
       "      <td>1121</td>\n",
       "      <td>549</td>\n",
       "      <td>613</td>\n",
       "      <td>587</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>889</td>\n",
       "      <td>939</td>\n",
       "      <td>960</td>\n",
       "      <td>805</td>\n",
       "      <td>805</td>\n",
       "      <td>1143</td>\n",
       "      <td>1121</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>542</td>\n",
       "      <td>473</td>\n",
       "      <td>504</td>\n",
       "      <td>626</td>\n",
       "      <td>647</td>\n",
       "      <td>795</td>\n",
       "      <td>832</td>\n",
       "      <td>366</td>\n",
       "      <td>288</td>\n",
       "      <td>318</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>542</td>\n",
       "      <td>473</td>\n",
       "      <td>504</td>\n",
       "      <td>626</td>\n",
       "      <td>647</td>\n",
       "      <td>795</td>\n",
       "      <td>832</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92</td>\n",
       "      <td>99</td>\n",
       "      <td>196</td>\n",
       "      <td>100</td>\n",
       "      <td>184</td>\n",
       "      <td>79</td>\n",
       "      <td>162</td>\n",
       "      <td>66</td>\n",
       "      <td>59</td>\n",
       "      <td>118</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92</td>\n",
       "      <td>99</td>\n",
       "      <td>196</td>\n",
       "      <td>100</td>\n",
       "      <td>184</td>\n",
       "      <td>79</td>\n",
       "      <td>162</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90</td>\n",
       "      <td>87</td>\n",
       "      <td>92</td>\n",
       "      <td>344</td>\n",
       "      <td>184</td>\n",
       "      <td>848</td>\n",
       "      <td>184</td>\n",
       "      <td>83</td>\n",
       "      <td>78</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90</td>\n",
       "      <td>87</td>\n",
       "      <td>92</td>\n",
       "      <td>344</td>\n",
       "      <td>184</td>\n",
       "      <td>848</td>\n",
       "      <td>184</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>169</td>\n",
       "      <td>98</td>\n",
       "      <td>101</td>\n",
       "      <td>90</td>\n",
       "      <td>96</td>\n",
       "      <td>95</td>\n",
       "      <td>185</td>\n",
       "      <td>141</td>\n",
       "      <td>68</td>\n",
       "      <td>85</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>169</td>\n",
       "      <td>98</td>\n",
       "      <td>101</td>\n",
       "      <td>90</td>\n",
       "      <td>96</td>\n",
       "      <td>95</td>\n",
       "      <td>185</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2    3    4     5     6    7    8    9  ...    68   69   70  \\\n",
       "0  889  939  960  805  805  1143  1121  549  613  587 ...   1.0  1.0  889   \n",
       "1  542  473  504  626  647   795   832  366  288  318 ...   1.0  1.0  542   \n",
       "2   92   99  196  100  184    79   162   66   59  118 ...   1.0  1.0   92   \n",
       "3   90   87   92  344  184   848   184   83   78   76 ...   1.0  1.0   90   \n",
       "4  169   98  101   90   96    95   185  141   68   85 ...   1.0  1.0  169   \n",
       "\n",
       "    71   72   73   74    75    76   77  \n",
       "0  939  960  805  805  1143  1121  1.0  \n",
       "1  473  504  626  647   795   832  1.0  \n",
       "2   99  196  100  184    79   162  0.0  \n",
       "3   87   92  344  184   848   184  1.0  \n",
       "4   98  101   90   96    95   185  1.0  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_org.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assigning the Predictor and Target Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140707, 77)\n",
      "(140707,)\n"
     ]
    }
   ],
   "source": [
    "X=data_org.loc[:,0:76]\n",
    "y=data_org.loc[:,77]\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Split the data into train and test samples, Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Libraries \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Randomizing the dataset and using 10% of the dataset for building models\n",
    "\n",
    "_,sample_data,_,sample_target = train_test_split(X, y, shuffle = True, test_size = 0.1)\n",
    "\n",
    "# Splitting data train / test ratio of 80:20                                      \n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_data, sample_target, random_state = 0, test_size = 0.2)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Voting Decision Tree and KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard Voting simply aggregate the predictions of each classifier and predict the class that gets the most votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DecisionTreeClassifier : 0.9452930728241563\n",
      "Accuracy of KNeighborsClassifier : 0.9573712255772646\n",
      "Accuracy of VotingClassifier : 0.9523978685612788\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Train Logistic Regression\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#Train KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "#Train voting classifer with Logistic Regression and Random Forest models\n",
    "voting_clf = VotingClassifier(estimators=[('Tree', tree),('kNN', knn)], voting='hard')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "for clf in (tree,knn, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Accuracy of {} : {}\".format(clf.__class__.__name__, accuracy_score(y_test, y_pred)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Voting Logistic Regression and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  For Soft Voting change the argument voting ='Soft'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soft Voting predict the class with the highest class probability, averaged over all the individual classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LogisticRegression : 0.9534635879218473\n",
      "Accuracy of RandomForestClassifier : 0.9680284191829485\n",
      "Accuracy of VotingClassifier : 0.9644760213143873\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Train Logistic Regression\n",
    "log_clf = LogisticRegression()\n",
    "log_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#Train Random Forest\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "#Train voting classifer with Logistic Regression and Random Forest models\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('RF', rnd_clf)],voting='soft')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "for clf in (log_clf,rnd_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Accuracy of {} : {}\".format(clf.__class__.__name__, accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning: Soft Voting works slightly better than Hard voting for our data considering 3 decimals of accuracy.\n",
    "\n",
    "#### This might be beacuse soft voting gives more weight to highly probable votes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging with Decision Trees and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DecisionTreeClassifier : 0.9466950942663731\n",
      "Accuracy of DecisionTreeClassifier with Bagging : 0.9669626998223801\n",
      "Accuracy of SVC : 0.899164649932574\n",
      "Accuracy of SVC with Bagging : 0.7996447602131439\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from  sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=0)\n",
    "svc_clf = SVC()\n",
    "\n",
    "clf_array = [dt_clf, svc_clf]\n",
    "for clf in clf_array:\n",
    "    reg_clf = cross_val_score(clf, X_train, y_train, cv=5, n_jobs=-1)\n",
    "    bag_clf = BaggingClassifier(clf,n_estimators=100, max_samples=100, bootstrap=True, n_jobs=-1, random_state=0)\n",
    "    bag_clf.fit(X_train, y_train)\n",
    "    y_pred = bag_clf.predict(X_test)\n",
    "    print(\"Accuracy of {} : {}\".format(clf.__class__.__name__,reg_clf.mean()))\n",
    "    print(\"Accuracy of {} with Bagging : {}\".format(clf.__class__.__name__, accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning:Accuracy of models created with bagging have higher accuracy scores  \n",
    "#### It is less overfitted than a single individual model   \n",
    "#### This is because bagging helps decrease the variance of the classifier and reduce overfitting, by resampling data from the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pasting with Decision Trees and SVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasting and Bagging are very similar, the main difference being that Bagging samples with replacement (which is called \"bootstrapping\") while Pasting samples without replacement.\n",
    "\n",
    "#### For pasting change argument to bootstrap=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DecisionTreeClassifier : 0.9466950942663731\n",
      "Accuracy of DecisionTreeClassifier with Pasting : 0.7996447602131439\n",
      "Accuracy of SVC : 0.899164649932574\n",
      "Accuracy of SVC with Pasting : 0.7996447602131439\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from  sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=0)\n",
    "svc_clf = SVC()\n",
    "\n",
    "clf_array = [dt_clf, svc_clf]\n",
    "for clf in clf_array:\n",
    "    reg_clf = cross_val_score(clf, X_train, y_train, cv=5, n_jobs=-1)\n",
    "    bagg_clf = BaggingClassifier(clf,n_estimators=100, max_samples=100, bootstrap=False, n_jobs=-1, random_state=0)\n",
    "    bag_clf.fit(X_train, y_train)\n",
    "    y_pred = bag_clf.predict(X_test)\n",
    "    print(\"Accuracy of {} : {}\".format(clf.__class__.__name__,reg_clf.mean()))\n",
    "    print(\"Accuracy of {} with Pasting : {}\".format(clf.__class__.__name__, accuracy_score(y_test, y_pred)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ada Boosting with Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.972\n",
      "Accuracy on test set: 0.963\n",
      "Accuracy of Ada boost with Decision Tree: 0.9626998223801065\n"
     ]
    }
   ],
   "source": [
    "# Import the Library\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "#Train Ada boost classifier object\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200, algorithm=\"SAMME.R\", learning_rate=0.5, random_state=0)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "ada_y_pred=ada_clf.predict(X_test)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(ada_clf.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(ada_clf.score(X_test, y_test)))\n",
    "print(\"Accuracy of Ada boost with Decision Tree:\",metrics.accuracy_score(y_test, ada_y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada Boosting with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.828\n",
      "Accuracy on test set: 0.824\n",
      "Accuracy of Ada Boost with Logistic Regression: 0.8238010657193605\n"
     ]
    }
   ],
   "source": [
    "# Import Support Vector Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logit=LogisticRegression()\n",
    "\n",
    "\n",
    "# Create adaboost classifer object\n",
    "ada_clf =AdaBoostClassifier(n_estimators=50,base_estimator=logit,learning_rate=1)\n",
    "\n",
    "# Train Adaboost Classifer\n",
    "ada_clf.fit(X_train, y_train)\n",
    "model_pred=ada_clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(ada_clf.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(ada_clf.score(X_test, y_test)))\n",
    "print(\"Accuracy of Ada Boost with Logistic Regression:\",metrics.accuracy_score(y_test, model_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.5\n",
      "Accuracy score on training set : 0.971\n",
      "Accuracy score on testing set : 0.966\n",
      "Accuracy of Gradient Boost : 0.9655417406749556\n",
      "Learning rate:  1\n",
      "Accuracy score on training set : 0.972\n",
      "Accuracy score on testing set : 0.964\n",
      "Accuracy of Gradient Boost : 0.9641207815275311\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "learning_rates = [0.5, 1]\n",
    "for learning_rate in learning_rates:\n",
    "    gb = GradientBoostingClassifier(n_estimators=20, learning_rate = learning_rate, max_depth = 2, random_state = 0)\n",
    "    gb.fit(X_train, y_train)\n",
    "    model_pred=gb.predict(X_test)\n",
    "    print(\"Learning rate: \", learning_rate)\n",
    "    print(\"Accuracy score on training set : {0:.3f}\".format(gb.score(X_train, y_train)))\n",
    "    print(\"Accuracy score on testing set : {0:.3f}\".format(gb.score(X_test, y_test)))\n",
    "    print(\"Accuracy of Gradient Boost :\",metrics.accuracy_score(y_test, model_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of explained variance [0.51804474 0.18075515]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_train)\n",
    "X_pca = pca.transform(X_train)\n",
    "\n",
    "#print(\"Pca components are {}\".format(pca.components_))\n",
    "print(\"Proportion of explained variance {}\".format(pca.explained_variance_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pca components PCA1 and PCA 2 accounts for about 67% of the total variation in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape:    (140707, 77)\n",
      "Transformed shape: (11256, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"original shape:   \", X.shape)\n",
    "print(\"Transformed shape:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the above result we understand that by projecting data into lower dimensional space \n",
    "#### Just 2 features and 11256 instances are sufficient enough to account for maximal variance in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca = PCA().fit(X_train)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This curve quantifies how much of the total, 77-dimensional variance is contained within the first N components.  see that with 5-6 components contain approximately 90% of the variance, while you need around 10 components to describe close to 100% of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity all the models are trained using 0.95 variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Make an instance of the Model\n",
    "pca = PCA(.95)\n",
    "pca.fit(X_train)\n",
    "\n",
    "train_img = pca.transform(X_train)\n",
    "test_img = pca.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree with PCA   Hyperparameters:max_depth=5,criterion='gini'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on training set : 0.961\n",
      "Accuracy score on testing set : 0.956\n"
     ]
    }
   ],
   "source": [
    "# Import Decision Tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Decision Tree with default parameters Gini Index,Max_depth=0\n",
    "dtree_tuned = DecisionTreeClassifier(random_state=0,max_depth=5,splitter='best',criterion='gini')\n",
    "dtree= DecisionTreeClassifier(random_state=0,max_depth=5,splitter='best',criterion='gini')\n",
    "dtree_tuned.fit(train_img, y_train)\n",
    "dtree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy score on training set : {0:.3f}\".format(dtree_tuned.score(train_img, y_train)))\n",
    "print(\"Accuracy score on testing set : {0:.3f}\".format(dtree_tuned.score(test_img, y_test)))\n",
    "\n",
    "report_table = [['Decision Tree', 'Gini Index ,Max depth=5',dtree.score(X_train, y_train),dtree.score(X_test, y_test)]]\n",
    "report_table =report_table+ [['Decision Tree with PCA', 'Gini Index ,Max depth=5',dtree_tuned.score(train_img, y_train),dtree_tuned.score(test_img, y_test)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with PCA  Hyperparameters : C1 ,L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on training set : 0.958\n",
      "Accuracy score on testing set : 0.956\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression(penalty = 'l1', C = 1)\n",
    "log = LogisticRegression(penalty = 'l1', C = 1)\n",
    "log.fit(X_train,y_train)\n",
    "logisticRegr.fit(train_img,y_train)\n",
    "\n",
    "print(\"Accuracy score on training set : {0:.3f}\".format(logisticRegr.score(train_img, y_train)))\n",
    "print(\"Accuracy score on testing set : {0:.3f}\".format(logisticRegr.score(test_img, y_test)))\n",
    "\n",
    "report_table =report_table +[['Logistic Regression','L1,C=1',log.score(X_train,y_train),log.score(X_test,y_test)]]\n",
    "report_table =report_table +[['Logistic Regression with PCA','L1,C=1',logisticRegr.score(train_img,y_train),logisticRegr.score(test_img,y_test)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbors with PCA Hyperparameters : K=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on training set: 0.963\n",
      "Accuracy score on testing set : 0.952\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "knn.fit(train_img, y_train)\n",
    "\n",
    "knn_clf=KNeighborsClassifier(n_neighbors=4)\n",
    "knn_clf.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "print('Accuracy score on training set: {:.3f}'.format(knn.score(train_img, y_train)))\n",
    "print('Accuracy score on testing set : {:.3f}'.format(knn.score(test_img, y_test)))\n",
    "\n",
    "report_table =report_table + [['KNN', 'K=4',knn_clf.score(X_train, y_train),knn_clf.score(X_test, y_test)]]\n",
    "report_table =report_table + [['KNN With PCA', 'K=4',knn.score(train_img, y_train),knn.score(test_img, y_test)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM with PCA Hyperparameters :C=10, Gamma=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on training set: 0.958\n",
      "Accuracy score on testing set : 0.957\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc_lin=SVC(kernel='linear',C=10,gamma=1)\n",
    "svc_lin.fit(train_img,y_train)\n",
    "\n",
    "svc=SVC(kernel='linear',C=10,gamma=1)\n",
    "svc.fit(X_train,y_train)\n",
    "\n",
    "print('Accuracy score on training set: {:.3f}'.format(svc_lin.score(train_img, y_train)))\n",
    "print('Accuracy score on testing set : {:.3f}'.format(svc_lin.score(test_img, y_test)))\n",
    "\n",
    "\n",
    "report_table = report_table + [['LinearSVC', 'C = 10', svc.score(X_train, y_train), svc.score(X_test, y_test)]]\n",
    "report_table = report_table + [['LinearSVC with PCA', 'C = 10', svc_lin.score(train_img, y_train), svc_lin.score(test_img, y_test)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model name</th>\n",
       "      <th>Model parameter</th>\n",
       "      <th>Train accuracy</th>\n",
       "      <th>Test accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>Gini Index ,Max depth=5</td>\n",
       "      <td>0.971393</td>\n",
       "      <td>0.963055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree with PCA</th>\n",
       "      <td>Decision Tree with PCA</td>\n",
       "      <td>Gini Index ,Max depth=5</td>\n",
       "      <td>0.960554</td>\n",
       "      <td>0.955950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>L1,C=1</td>\n",
       "      <td>0.963486</td>\n",
       "      <td>0.966252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression with PCA</th>\n",
       "      <td>Logistic Regression with PCA</td>\n",
       "      <td>L1,C=1</td>\n",
       "      <td>0.957534</td>\n",
       "      <td>0.955595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>KNN</td>\n",
       "      <td>K=4</td>\n",
       "      <td>0.967484</td>\n",
       "      <td>0.956661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN With PCA</th>\n",
       "      <td>KNN With PCA</td>\n",
       "      <td>K=4</td>\n",
       "      <td>0.962598</td>\n",
       "      <td>0.952398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>C = 10</td>\n",
       "      <td>0.965441</td>\n",
       "      <td>0.966252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC with PCA</th>\n",
       "      <td>LinearSVC with PCA</td>\n",
       "      <td>C = 10</td>\n",
       "      <td>0.958156</td>\n",
       "      <td>0.956661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Model name  \\\n",
       "Model name                                                   \n",
       "Decision Tree                                Decision Tree   \n",
       "Decision Tree with PCA              Decision Tree with PCA   \n",
       "Logistic Regression                    Logistic Regression   \n",
       "Logistic Regression with PCA  Logistic Regression with PCA   \n",
       "KNN                                                    KNN   \n",
       "KNN With PCA                                  KNN With PCA   \n",
       "LinearSVC                                        LinearSVC   \n",
       "LinearSVC with PCA                      LinearSVC with PCA   \n",
       "\n",
       "                                      Model parameter  Train accuracy  \\\n",
       "Model name                                                              \n",
       "Decision Tree                 Gini Index ,Max depth=5        0.971393   \n",
       "Decision Tree with PCA        Gini Index ,Max depth=5        0.960554   \n",
       "Logistic Regression                            L1,C=1        0.963486   \n",
       "Logistic Regression with PCA                   L1,C=1        0.957534   \n",
       "KNN                                               K=4        0.967484   \n",
       "KNN With PCA                                      K=4        0.962598   \n",
       "LinearSVC                                      C = 10        0.965441   \n",
       "LinearSVC with PCA                             C = 10        0.958156   \n",
       "\n",
       "                              Test accuracy  \n",
       "Model name                                   \n",
       "Decision Tree                      0.963055  \n",
       "Decision Tree with PCA             0.955950  \n",
       "Logistic Regression                0.966252  \n",
       "Logistic Regression with PCA       0.955595  \n",
       "KNN                                0.956661  \n",
       "KNN With PCA                       0.952398  \n",
       "LinearSVC                          0.966252  \n",
       "LinearSVC with PCA                 0.956661  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = pd.DataFrame(report_table,columns = ['Model name', 'Model parameter', 'Train accuracy', 'Test accuracy'])\n",
    "report.index = report['Model name']\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Learning : From the above table it is clearly evident that the model accuracy improved on using PCA for dimension reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_dim parameter : needed only for first layer , we have 77 input variables\n",
    "Activation function : must be as close as possible to 1. I have chosen rectifier function in hidden layer and sigmoid function in the output layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "11256/11256 [==============================] - 1s 103us/step - loss: 0.2223 - acc: 0.9120\n",
      "Epoch 2/150\n",
      "11256/11256 [==============================] - 1s 75us/step - loss: 0.1080 - acc: 0.9574\n",
      "Epoch 3/150\n",
      "11256/11256 [==============================] - 1s 72us/step - loss: 0.1040 - acc: 0.9581\n",
      "Epoch 4/150\n",
      "11256/11256 [==============================] - 1s 70us/step - loss: 0.1014 - acc: 0.9607\n",
      "Epoch 5/150\n",
      "11256/11256 [==============================] - 1s 69us/step - loss: 0.1003 - acc: 0.9621\n",
      "Epoch 6/150\n",
      "11256/11256 [==============================] - 1s 70us/step - loss: 0.0982 - acc: 0.9630\n",
      "Epoch 7/150\n",
      "11256/11256 [==============================] - 1s 71us/step - loss: 0.0984 - acc: 0.9620\n",
      "Epoch 8/150\n",
      "11256/11256 [==============================] - 1s 70us/step - loss: 0.0970 - acc: 0.9625\n",
      "Epoch 9/150\n",
      "11256/11256 [==============================] - 1s 75us/step - loss: 0.0958 - acc: 0.9620\n",
      "Epoch 10/150\n",
      "11256/11256 [==============================] - 1s 77us/step - loss: 0.0956 - acc: 0.9622\n",
      "Epoch 11/150\n",
      "11256/11256 [==============================] - 1s 72us/step - loss: 0.0945 - acc: 0.9638\n",
      "Epoch 12/150\n",
      "11256/11256 [==============================] - 1s 71us/step - loss: 0.0939 - acc: 0.9633\n",
      "Epoch 13/150\n",
      "11256/11256 [==============================] - 1s 82us/step - loss: 0.0936 - acc: 0.9638\n",
      "Epoch 14/150\n",
      "11256/11256 [==============================] - 1s 72us/step - loss: 0.0943 - acc: 0.9645\n",
      "Epoch 15/150\n",
      "11256/11256 [==============================] - 1s 78us/step - loss: 0.0937 - acc: 0.9630\n",
      "Epoch 16/150\n",
      "11256/11256 [==============================] - 1s 78us/step - loss: 0.0932 - acc: 0.9639\n",
      "Epoch 17/150\n",
      "11256/11256 [==============================] - 1s 78us/step - loss: 0.0928 - acc: 0.9634\n",
      "Epoch 18/150\n",
      "11256/11256 [==============================] - 1s 69us/step - loss: 0.0937 - acc: 0.9649\n",
      "Epoch 19/150\n",
      "11256/11256 [==============================] - 1s 73us/step - loss: 0.0925 - acc: 0.9638\n",
      "Epoch 20/150\n",
      "11256/11256 [==============================] - 1s 78us/step - loss: 0.0924 - acc: 0.9645\n",
      "Epoch 21/150\n",
      "11256/11256 [==============================] - 1s 74us/step - loss: 0.0930 - acc: 0.9632\n",
      "Epoch 22/150\n",
      "11256/11256 [==============================] - 1s 72us/step - loss: 0.0923 - acc: 0.9644\n",
      "Epoch 23/150\n",
      "11256/11256 [==============================] - 1s 72us/step - loss: 0.0926 - acc: 0.9635\n",
      "Epoch 24/150\n",
      "11256/11256 [==============================] - 1s 70us/step - loss: 0.0919 - acc: 0.9640\n",
      "Epoch 25/150\n",
      "11256/11256 [==============================] - 1s 74us/step - loss: 0.0912 - acc: 0.9655\n",
      "Epoch 26/150\n",
      "11256/11256 [==============================] - 1s 70us/step - loss: 0.0912 - acc: 0.9655\n",
      "Epoch 27/150\n",
      "11256/11256 [==============================] - 1s 71us/step - loss: 0.0914 - acc: 0.9646\n",
      "Epoch 28/150\n",
      "11256/11256 [==============================] - 1s 71us/step - loss: 0.0923 - acc: 0.9638\n",
      "Epoch 29/150\n",
      "11256/11256 [==============================] - 1s 70us/step - loss: 0.0911 - acc: 0.9651\n",
      "Epoch 30/150\n",
      "11256/11256 [==============================] - 1s 71us/step - loss: 0.0921 - acc: 0.9627\n",
      "Epoch 31/150\n",
      "11256/11256 [==============================] - 1s 71us/step - loss: 0.0920 - acc: 0.9630\n",
      "Epoch 32/150\n",
      "11256/11256 [==============================] - 1s 69us/step - loss: 0.0908 - acc: 0.9648\n",
      "Epoch 33/150\n",
      "11256/11256 [==============================] - 1s 69us/step - loss: 0.0918 - acc: 0.9638\n",
      "Epoch 34/150\n",
      "11256/11256 [==============================] - 1s 70us/step - loss: 0.0910 - acc: 0.9633\n",
      "Epoch 35/150\n",
      "11256/11256 [==============================] - 1s 70us/step - loss: 0.0912 - acc: 0.9648\n",
      "Epoch 36/150\n",
      "11256/11256 [==============================] - 1s 70us/step - loss: 0.0913 - acc: 0.9648\n",
      "Epoch 37/150\n",
      "11256/11256 [==============================] - 1s 71us/step - loss: 0.0911 - acc: 0.9647\n",
      "Epoch 38/150\n",
      "11256/11256 [==============================] - 1s 70us/step - loss: 0.0905 - acc: 0.9643\n",
      "Epoch 39/150\n",
      "11256/11256 [==============================] - 1s 72us/step - loss: 0.0896 - acc: 0.9642\n",
      "Epoch 40/150\n",
      "11256/11256 [==============================] - 1s 72us/step - loss: 0.0911 - acc: 0.9651\n",
      "Epoch 41/150\n",
      "11256/11256 [==============================] - 1s 70us/step - loss: 0.0896 - acc: 0.9653\n",
      "Epoch 42/150\n",
      "11256/11256 [==============================] - 1s 69us/step - loss: 0.0903 - acc: 0.9647\n",
      "Epoch 43/150\n",
      "11256/11256 [==============================] - 1s 71us/step - loss: 0.0907 - acc: 0.9640\n",
      "Epoch 44/150\n",
      "11256/11256 [==============================] - 1s 72us/step - loss: 0.0906 - acc: 0.9648\n",
      "Epoch 45/150\n",
      "11256/11256 [==============================] - 1s 70us/step - loss: 0.0899 - acc: 0.9654\n",
      "Epoch 46/150\n",
      "11256/11256 [==============================] - 1s 74us/step - loss: 0.0905 - acc: 0.9657\n",
      "Epoch 47/150\n",
      "11256/11256 [==============================] - 1s 70us/step - loss: 0.0904 - acc: 0.9642\n",
      "Epoch 48/150\n",
      "11256/11256 [==============================] - 1s 74us/step - loss: 0.0896 - acc: 0.9660\n",
      "Epoch 49/150\n",
      "11256/11256 [==============================] - 1s 73us/step - loss: 0.0894 - acc: 0.9650\n",
      "Epoch 50/150\n",
      "11256/11256 [==============================] - 1s 73us/step - loss: 0.0898 - acc: 0.9629\n",
      "Epoch 51/150\n",
      "11256/11256 [==============================] - 1s 73us/step - loss: 0.0901 - acc: 0.9650\n",
      "Epoch 52/150\n",
      "11256/11256 [==============================] - 1s 81us/step - loss: 0.0903 - acc: 0.9643\n",
      "Epoch 53/150\n",
      "11256/11256 [==============================] - 1s 69us/step - loss: 0.0890 - acc: 0.9658\n",
      "Epoch 54/150\n",
      "11256/11256 [==============================] - 1s 73us/step - loss: 0.0894 - acc: 0.9646\n",
      "Epoch 55/150\n",
      "11256/11256 [==============================] - 1s 78us/step - loss: 0.0905 - acc: 0.9646\n",
      "Epoch 56/150\n",
      "11256/11256 [==============================] - 1s 72us/step - loss: 0.0897 - acc: 0.9644\n",
      "Epoch 57/150\n",
      "11256/11256 [==============================] - 1s 76us/step - loss: 0.0900 - acc: 0.9641\n",
      "Epoch 58/150\n",
      "11256/11256 [==============================] - 1s 80us/step - loss: 0.0893 - acc: 0.9646\n",
      "Epoch 59/150\n",
      "11256/11256 [==============================] - 1s 70us/step - loss: 0.0904 - acc: 0.9654\n",
      "Epoch 60/150\n",
      "11256/11256 [==============================] - 1s 74us/step - loss: 0.0893 - acc: 0.9648\n",
      "Epoch 61/150\n",
      "11256/11256 [==============================] - 1s 76us/step - loss: 0.0888 - acc: 0.9646\n",
      "Epoch 62/150\n",
      "11256/11256 [==============================] - 1s 75us/step - loss: 0.0888 - acc: 0.9648\n",
      "Epoch 63/150\n",
      "11256/11256 [==============================] - 1s 75us/step - loss: 0.0887 - acc: 0.9654\n",
      "Epoch 64/150\n",
      "11256/11256 [==============================] - 1s 73us/step - loss: 0.0889 - acc: 0.9655\n",
      "Epoch 65/150\n",
      "11256/11256 [==============================] - 1s 72us/step - loss: 0.0884 - acc: 0.9651\n",
      "Epoch 66/150\n",
      "11256/11256 [==============================] - 1s 72us/step - loss: 0.0890 - acc: 0.9650\n",
      "Epoch 67/150\n",
      "11256/11256 [==============================] - 1s 74us/step - loss: 0.0885 - acc: 0.9651\n",
      "Epoch 68/150\n",
      "11256/11256 [==============================] - 1s 73us/step - loss: 0.0881 - acc: 0.9662\n",
      "Epoch 69/150\n",
      "11256/11256 [==============================] - 1s 73us/step - loss: 0.0886 - acc: 0.9653\n",
      "Epoch 70/150\n",
      "11256/11256 [==============================] - 1s 78us/step - loss: 0.0884 - acc: 0.9662\n",
      "Epoch 71/150\n",
      "11256/11256 [==============================] - 1s 72us/step - loss: 0.0890 - acc: 0.9650\n",
      "Epoch 72/150\n",
      "11256/11256 [==============================] - 1s 80us/step - loss: 0.0885 - acc: 0.9662\n",
      "Epoch 73/150\n",
      "11256/11256 [==============================] - 1s 74us/step - loss: 0.0896 - acc: 0.9651\n",
      "Epoch 74/150\n",
      "11256/11256 [==============================] - 1s 80us/step - loss: 0.0877 - acc: 0.9643\n",
      "Epoch 75/150\n",
      "11256/11256 [==============================] - 1s 81us/step - loss: 0.0889 - acc: 0.9640\n",
      "Epoch 76/150\n",
      "11256/11256 [==============================] - 1s 83us/step - loss: 0.0892 - acc: 0.9653\n",
      "Epoch 77/150\n",
      "11256/11256 [==============================] - 1s 74us/step - loss: 0.0887 - acc: 0.9662\n",
      "Epoch 78/150\n",
      "11256/11256 [==============================] - 1s 78us/step - loss: 0.0887 - acc: 0.9654\n",
      "Epoch 79/150\n",
      "11256/11256 [==============================] - 1s 82us/step - loss: 0.0887 - acc: 0.9654\n",
      "Epoch 80/150\n",
      "11256/11256 [==============================] - 1s 77us/step - loss: 0.0880 - acc: 0.9666\n",
      "Epoch 81/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11256/11256 [==============================] - 1s 75us/step - loss: 0.0879 - acc: 0.9653\n",
      "Epoch 82/150\n",
      "11256/11256 [==============================] - 1s 83us/step - loss: 0.0879 - acc: 0.9654\n",
      "Epoch 83/150\n",
      "11256/11256 [==============================] - 1s 112us/step - loss: 0.0876 - acc: 0.9657\n",
      "Epoch 84/150\n",
      "11256/11256 [==============================] - 1s 93us/step - loss: 0.0888 - acc: 0.9646\n",
      "Epoch 85/150\n",
      "11256/11256 [==============================] - 1s 78us/step - loss: 0.0882 - acc: 0.9655\n",
      "Epoch 86/150\n",
      "11256/11256 [==============================] - 1s 80us/step - loss: 0.0882 - acc: 0.9648\n",
      "Epoch 87/150\n",
      "11256/11256 [==============================] - 1s 80us/step - loss: 0.0877 - acc: 0.9659\n",
      "Epoch 88/150\n",
      "11256/11256 [==============================] - 1s 84us/step - loss: 0.0882 - acc: 0.9654\n",
      "Epoch 89/150\n",
      "11256/11256 [==============================] - 1s 77us/step - loss: 0.0880 - acc: 0.9648\n",
      "Epoch 90/150\n",
      "11256/11256 [==============================] - 1s 76us/step - loss: 0.0877 - acc: 0.9661\n",
      "Epoch 91/150\n",
      "11256/11256 [==============================] - 1s 74us/step - loss: 0.0883 - acc: 0.9649\n",
      "Epoch 92/150\n",
      "11256/11256 [==============================] - ETA: 0s - loss: 0.0879 - acc: 0.964 - 1s 74us/step - loss: 0.0876 - acc: 0.9646\n",
      "Epoch 93/150\n",
      "11256/11256 [==============================] - 1s 73us/step - loss: 0.0880 - acc: 0.9666\n",
      "Epoch 94/150\n",
      "11256/11256 [==============================] - 1s 73us/step - loss: 0.0880 - acc: 0.9659\n",
      "Epoch 95/150\n",
      "11256/11256 [==============================] - 1s 71us/step - loss: 0.0880 - acc: 0.9666\n",
      "Epoch 96/150\n",
      "11256/11256 [==============================] - 1s 73us/step - loss: 0.0873 - acc: 0.9658\n",
      "Epoch 97/150\n",
      "11256/11256 [==============================] - 1s 72us/step - loss: 0.0875 - acc: 0.9659\n",
      "Epoch 98/150\n",
      "11256/11256 [==============================] - 1s 84us/step - loss: 0.0876 - acc: 0.9657\n",
      "Epoch 99/150\n",
      "11256/11256 [==============================] - 1s 80us/step - loss: 0.0882 - acc: 0.9660\n",
      "Epoch 100/150\n",
      "11256/11256 [==============================] - 1s 70us/step - loss: 0.0876 - acc: 0.9648\n",
      "Epoch 101/150\n",
      "11256/11256 [==============================] - 1s 75us/step - loss: 0.0881 - acc: 0.9650\n",
      "Epoch 102/150\n",
      "11256/11256 [==============================] - 1s 72us/step - loss: 0.0878 - acc: 0.9655\n",
      "Epoch 103/150\n",
      "11256/11256 [==============================] - 1s 76us/step - loss: 0.0876 - acc: 0.9662\n",
      "Epoch 104/150\n",
      "11256/11256 [==============================] - 1s 73us/step - loss: 0.0874 - acc: 0.9660\n",
      "Epoch 105/150\n",
      "11256/11256 [==============================] - 1s 71us/step - loss: 0.0873 - acc: 0.9655\n",
      "Epoch 106/150\n",
      "11256/11256 [==============================] - 1s 70us/step - loss: 0.0872 - acc: 0.9650\n",
      "Epoch 107/150\n",
      "11256/11256 [==============================] - 1s 69us/step - loss: 0.0876 - acc: 0.9658\n",
      "Epoch 108/150\n",
      "11256/11256 [==============================] - 1s 75us/step - loss: 0.0877 - acc: 0.9665\n",
      "Epoch 109/150\n",
      "11256/11256 [==============================] - 1s 86us/step - loss: 0.0873 - acc: 0.9667\n",
      "Epoch 110/150\n",
      "11256/11256 [==============================] - 1s 71us/step - loss: 0.0880 - acc: 0.9658\n",
      "Epoch 111/150\n",
      "11256/11256 [==============================] - 1s 74us/step - loss: 0.0873 - acc: 0.9657\n",
      "Epoch 112/150\n",
      "11256/11256 [==============================] - 1s 86us/step - loss: 0.0872 - acc: 0.9654: 1s - loss: 0.\n",
      "Epoch 113/150\n",
      "11256/11256 [==============================] - 1s 78us/step - loss: 0.0866 - acc: 0.9663: 0s - loss: 0.0713\n",
      "Epoch 114/150\n",
      "11256/11256 [==============================] - 1s 78us/step - loss: 0.0879 - acc: 0.9662\n",
      "Epoch 115/150\n",
      "11256/11256 [==============================] - 1s 88us/step - loss: 0.0882 - acc: 0.9650\n",
      "Epoch 116/150\n",
      "11256/11256 [==============================] - 1s 80us/step - loss: 0.0870 - acc: 0.9673\n",
      "Epoch 117/150\n",
      "11256/11256 [==============================] - 1s 76us/step - loss: 0.0863 - acc: 0.9667\n",
      "Epoch 118/150\n",
      "11256/11256 [==============================] - 1s 77us/step - loss: 0.0863 - acc: 0.9678\n",
      "Epoch 119/150\n",
      "11256/11256 [==============================] - 1s 73us/step - loss: 0.0871 - acc: 0.9662\n",
      "Epoch 120/150\n",
      "11256/11256 [==============================] - 1s 85us/step - loss: 0.0875 - acc: 0.9662\n",
      "Epoch 121/150\n",
      "11256/11256 [==============================] - 1s 83us/step - loss: 0.0875 - acc: 0.9663\n",
      "Epoch 122/150\n",
      "11256/11256 [==============================] - 1s 80us/step - loss: 0.0868 - acc: 0.9647\n",
      "Epoch 123/150\n",
      "11256/11256 [==============================] - 1s 80us/step - loss: 0.0869 - acc: 0.9662\n",
      "Epoch 124/150\n",
      "11256/11256 [==============================] - 1s 69us/step - loss: 0.0865 - acc: 0.9670\n",
      "Epoch 125/150\n",
      "11256/11256 [==============================] - 1s 68us/step - loss: 0.0872 - acc: 0.9662\n",
      "Epoch 126/150\n",
      "11256/11256 [==============================] - 1s 70us/step - loss: 0.0860 - acc: 0.9666\n",
      "Epoch 127/150\n",
      "11256/11256 [==============================] - 1s 65us/step - loss: 0.0875 - acc: 0.9652\n",
      "Epoch 128/150\n",
      "11256/11256 [==============================] - 1s 65us/step - loss: 0.0866 - acc: 0.9661\n",
      "Epoch 129/150\n",
      "11256/11256 [==============================] - 1s 64us/step - loss: 0.0865 - acc: 0.9665\n",
      "Epoch 130/150\n",
      "11256/11256 [==============================] - 1s 65us/step - loss: 0.0873 - acc: 0.9652\n",
      "Epoch 131/150\n",
      "11256/11256 [==============================] - 1s 73us/step - loss: 0.0863 - acc: 0.9659\n",
      "Epoch 132/150\n",
      "11256/11256 [==============================] - 1s 68us/step - loss: 0.0870 - acc: 0.9652\n",
      "Epoch 133/150\n",
      "11256/11256 [==============================] - 1s 93us/step - loss: 0.0864 - acc: 0.9662: 0s - loss: 0.0\n",
      "Epoch 134/150\n",
      "11256/11256 [==============================] - 1s 72us/step - loss: 0.0863 - acc: 0.9670\n",
      "Epoch 135/150\n",
      "11256/11256 [==============================] - 1s 69us/step - loss: 0.0866 - acc: 0.9657\n",
      "Epoch 136/150\n",
      "11256/11256 [==============================] - 1s 68us/step - loss: 0.0868 - acc: 0.9653\n",
      "Epoch 137/150\n",
      "11256/11256 [==============================] - 1s 74us/step - loss: 0.0867 - acc: 0.9658\n",
      "Epoch 138/150\n",
      "11256/11256 [==============================] - 1s 71us/step - loss: 0.0870 - acc: 0.9666\n",
      "Epoch 139/150\n",
      "11256/11256 [==============================] - 1s 64us/step - loss: 0.0866 - acc: 0.9652\n",
      "Epoch 140/150\n",
      "11256/11256 [==============================] - 1s 74us/step - loss: 0.0866 - acc: 0.9660\n",
      "Epoch 141/150\n",
      "11256/11256 [==============================] - ETA: 0s - loss: 0.0871 - acc: 0.966 - 1s 75us/step - loss: 0.0870 - acc: 0.9662\n",
      "Epoch 142/150\n",
      "11256/11256 [==============================] - 1s 69us/step - loss: 0.0863 - acc: 0.9655\n",
      "Epoch 143/150\n",
      "11256/11256 [==============================] - 1s 67us/step - loss: 0.0864 - acc: 0.9651\n",
      "Epoch 144/150\n",
      "11256/11256 [==============================] - 1s 68us/step - loss: 0.0867 - acc: 0.9658\n",
      "Epoch 145/150\n",
      "11256/11256 [==============================] - 1s 64us/step - loss: 0.0860 - acc: 0.9665\n",
      "Epoch 146/150\n",
      "11256/11256 [==============================] - 1s 65us/step - loss: 0.0862 - acc: 0.9668\n",
      "Epoch 147/150\n",
      "11256/11256 [==============================] - 1s 71us/step - loss: 0.0857 - acc: 0.9664\n",
      "Epoch 148/150\n",
      "11256/11256 [==============================] - 1s 70us/step - loss: 0.0864 - acc: 0.9659\n",
      "Epoch 149/150\n",
      "11256/11256 [==============================] - 1s 64us/step - loss: 0.0865 - acc: 0.9654\n",
      "Epoch 150/150\n",
      "11256/11256 [==============================] - 1s 64us/step - loss: 0.0855 - acc: 0.9673\n",
      "2815/2815 [==============================] - 0s 25us/step\n",
      "Accuracy\n",
      "acc: 96.87%\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=77, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=10)\n",
    "\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary:\n",
    "While Voting - ensembler helps us combine the power of multiple model to improve prediction.\n",
    "Bagging can help in minimizing the variance by creating subsets of data\n",
    "Boosting reduces the possibility of misclassification of data \n",
    "PCA captures variability in data and provides a way to represent the data using minimal number of features and instances\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
